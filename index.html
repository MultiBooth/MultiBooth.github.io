<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <title>MultiBooth</title>
    <link href="./DreamBooth_files/style.css" rel="stylesheet" />
    <script
      type="text/javascript"
      src="./DreamBooth_files/jquery.mlens-1.0.min.js"
    ></script>
    <script type="text/javascript" src="./DreamBooth_files/jquery.js"></script>
  </head>

  <body>
    <div class="content">
      <h1>
        <strong
          >MultiBooth: Towards Generating All Your Concepts in an Image from
          Text</strong
        >
      </h1>
      <p id="authors">
        <span><a href="https://multibooth.github.io/"></a></span>
        <a href="https://multibooth.github.io/">Chenyang Zhu <sup>1</sup> </a>
        <a href="https://multibooth.github.io/">Kai Li <sup>2✝</sup> </a>
        <a href="https://multibooth.github.io/">Yue Ma <sup>1</sup> </a>
        <a href="https://multibooth.github.io/">Chunming He <sup>1</sup> </a>
        <a href="https://multibooth.github.io/">Xiu Li <sup>1✝</sup> </a><br />
        <br />
        <span style="font-size: 0.8em; margin-top: 0.5em">
          <a
            ><sup>1</sup>Tsinghua Shenzhen International Graduate School,
            Tsinghua University</a
          >
          <a><sup>2</sup>Meta Reality Labs</a>
        </span>
      </p>
      <br />
      <img
        src="./DreamBooth_files/Title-case.png"
        class="teaser-gif"
        style="width: 100%"
      /><br />
      <!-- <h3 style="text-align: center">
        <em
          >It’s like a photo booth, but once the subject is captured, it can be
          synthesized wherever your dreams take you…</em
        >
      </h3> -->
      <font size="+2">
        <p style="text-align: center">
          <a href="https://arxiv.org/abs/TODO" target="_blank">[Paper]</a>
          &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://github.com/TODO" target="_blank">[Code]</a>
          &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="DreamBooth_files/TODO" target="_blank">[BibTeX]</a>
        </p>
      </font>
    </div>
    <div class="content">
      <h2 style="text-align: center">Abstract</h2>
      <p>
        This paper introduces MultiBooth, a novel and efficient technique for
        multi-concept customization in image generation from text. Despite the
        significant advancements in customized generation methods, particularly
        with the success of diffusion models, existing methods often struggle
        with multi-concept scenarios due to low concept fidelity and high
        inference cost. MultiBooth addresses these issues by dividing the
        multi-concept generation process into two phases: a single-concept
        learning phase and a multi-concept integration phase. During the
        single-concept learning phase, we employ a multi-modal image encoder and
        an efficient concept encoding technique to learn a concise and
        discriminative representation for each concept. In the multi-concept
        integration phase, we use bounding boxes to define the generation area
        for each concept within the cross-attention map. This method enables the
        creation of individual concepts within their specified regions, thereby
        facilitating the formation of multi-concept images. This strategy not
        only improves concept fidelity but also reduces additional inference
        cost. MultiBooth surpasses various baselines in both qualitative and
        quantitative evaluations, showcasing its superior performance and
        computational efficiency.
      </p>
    </div>
    <div class="content">
      <h2>Approach</h2>
      <p>
        Given a series of images that represent some concepts of interest, the
        goal of multi-concept customization (MCC) is to generate images that
        include any number of concepts in various styles, contexts, layout
        relationship as specified by given text prompts.
      </p>
      <br />
      <img
        class="summary-img"
        src="./DreamBooth_files/Framework.png"
        style="width: 100%"
      />
      <br />
      <p>
        The overall pipeline of MultiBooth can be devided into two phase: (a)
        During the single-concept learning phase, a multi-modal encoder and LoRA
        parameters are trained to encode every single concept. (b) During the
        multi-concept integration phase, the customized embeddings $S^*$ and
        $V^*$ are converted into text embeddings, which are then combined with
        the corresponding LoRA parameters to form single-concept modules. These
        single-concept modules, along with the bounding boxes, are intended to
        serve as input for the regional customization module.
      </p>
      <!-- <br />
      <img
        class="summary-img"
        src="./DreamBooth_files/system.png"
        style="width: 100%"
      />
      <br /> -->
    </div>
    <div class="content">
      <h2>Results</h2>
      <!-- <p>
        Results for re-contextualization of a bag and vase subject instances. By
        finetuning a model using our method we are able to generate different
        images of the a subject instance in different environments, with high
        preservation of subject details and realistic interaction between the
        scene and the subject. We display the conditioning prompts below each
        image.
      </p> -->
      <img
        class="summary-img"
        src="./DreamBooth_files/main_compare.png"
        style="width: 100%"
      />
    </div>
    <!-- <div class="content">
      <h2>Art Rendition</h2>
      <p>
        Original artistic renditions of our subject dog in the style of famous
        painters. We remark that many of the generated poses were not seen in
        the training set, such as the Van Gogh and Warhol rendition. We also
        note that some renditions seem to have novel composition and faithfully
        imitate the style of the painter - even suggesting some sort of
        creativity (extrapolation given previous knowledge).
      </p>
      <br />
      <img
        class="summary-img"
        src="./DreamBooth_files/art.png"
        style="width: 100%"
      />
      <br />
    </div>
    <div class="content">
      <h2>Text-Guided View Synthesis</h2>
      <p>
        Our technique can synthesized images with specified viewpoints for a
        subject cat (left to right: top, bottom, side and back views). Note that
        the generated poses are different from the input poses, and the
        background changes in a realistic manner given a pose change. We also
        highlight the preservation of complex fur patterns on the subject cat's
        forehead.
      </p>
      <br />
      <img
        class="summary-img"
        src="./DreamBooth_files/novel_views.png"
        style="width: 100%"
      />
      <br />
    </div>
    <div class="content">
      <h2>Property Modification</h2>
      <p>
        We show color modifications in the first row (using prompts ``a [color]
        [V] car''), and crosses between a specific dog and different animals in
        the second row (using prompts ``a cross of a [V] dog and a [target
        species]''). We highlight the fact that our method preserves unique
        visual features that give the subject its identity or essence, while
        performing the required property modification.
      </p>
      <br />
      <img
        class="summary-img"
        src="./DreamBooth_files/property_modification.png"
        style="width: 100%"
      />
      <br />
    </div>
    <div class="content">
      <h2>Accessorization</h2>
      <p>
        Outfitting a dog with accessories. The identity of the subject is
        preserved and many different outfits or accessories can be applied to
        the dog given a prompt of type
        <em>"a [V] dog wearing a police/chef/witch outfit''</em>. We observe a
        realistic interaction between the subject dog and the outfits or
        accessories, as well as a large variety of possible options.
      </p>
      <br />
      <img
        class="summary-img"
        src="./DreamBooth_files/accessories.png"
        style="width: 100%"
      />
      <br />
    </div>
    <div class="content">
      <h2>Societal Impact</h2>
      <p>
        This project aims to provide users with an effective tool for
        synthesizing personal subjects (animals, objects) in different contexts.
        While general text-to-image models might be biased towards specific
        attributes when synthesizing images from text, our approach enables the
        user to get a better reconstruction of their desirable subjects. On
        contrary, malicious parties might try to use such images to mislead
        viewers. This is a common issue, existing in other generative models
        approaches or content manipulation techniques. Future research in
        generative modeling, and specifically of personalized generative priors,
        must continue investigating and revalidating these concerns.
      </p>
      <br />
    </div> -->
    <!-- <div class="content">
      <h2>BibTex</h2>
      <code>
        @article{ruiz2022dreambooth,<br />
        &nbsp;&nbsp;title={DreamBooth: Fine Tuning Text-to-image Diffusion
        Models for Subject-Driven Generation},<br />
        &nbsp;&nbsp;author={Ruiz, Nataniel and Li, Yuanzhen and Jampani, Varun
        and Pritch, Yael and Rubinstein, Michael and Aberman, Kfir},<br />
        &nbsp;&nbsp;booktitle={arXiv preprint arxiv:2208.12242},<br />
        &nbsp;&nbsp;year={2022}<br />
        }
      </code>
    </div> -->
    <!-- <div class="content" id="acknowledgements">
      <p>
        <strong>Acknowledgements</strong>: We thank Rinon Gal, Adi Zicher, Ron
        Mokady, Bill Freeman, Dilip Krishnan, Huiwen Chang and Daniel Cohen-Or
        for their valuable inputs that helped improve this work, and to Mohammad
        Norouzi, Chitwan Saharia and William Chan for providing us with their
        support and the pretrained models of Imagen. Finally, a special thank
        you to David Salesin for his feedback, advice and for his support for
        the project.
      </p>
    </div> -->
  </body>
</html>
